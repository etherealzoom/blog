<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RWKV on etherealzoom</title><link>https://etherealzoom.github.io/blog/categories/rwkv/</link><description>Recent content in RWKV on etherealzoom</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>etherealzoom</copyright><lastBuildDate>Wed, 09 Jul 2025 23:06:03 +0800</lastBuildDate><atom:link href="https://etherealzoom.github.io/blog/categories/rwkv/index.xml" rel="self" type="application/rss+xml"/><item><title>Quantization</title><link>https://etherealzoom.github.io/blog/p/quantization/</link><pubDate>Wed, 09 Jul 2025 23:06:03 +0800</pubDate><guid>https://etherealzoom.github.io/blog/p/quantization/</guid><description>&lt;h2 id="疑惑">疑惑
&lt;/h2>&lt;h3 id="pytorch平台上对rwkv的困惑度计算模块进行量化的经历">Pytorch平台上对RWKV的困惑度计算模块进行量化的经历
&lt;/h3>&lt;p>Q:更稳妥的方式是在模型加载和量化后，将整个模型转为float32，再量化后转换成float32的目的是什么呢，这样不是把原来量化好的给毁了吗&lt;br>
A：&lt;/p>
&lt;h4 id="pytorch上量化做了什么">pytorch上量化做了什么
&lt;/h4>&lt;p>当调用torch.quantization.quantize_dynamic时，PyTorch并不仅仅是把权重张量的数据类型（dtype）改了一下。它做了一件更根本的事：它将模型中的标准nn.Linear层替换成了一个全新的、专门的模块，叫做DynamicQuantizedLinear。&lt;br>
在这个新的DynamicQuantizedLinear模块内部，权重确实被压缩并以**torch.qint8（8位整型）**的格式存储了起来。&lt;br>
同时，这个模块还保存了将这些INT8整数反量化回浮点数所需的缩放因子（scale）和零点（zero-point）（它们是浮点数）。&lt;/p>
&lt;h4 id="totorchfloat32-到底做了什么">.to(torch.float32) 到底做了什么？
&lt;/h4>&lt;p>当您对一个模型调用.to(dtype)方法时，PyTorch会遍历模型的所有参数（parameters）和缓冲区（buffers），并尝试将它们的类型转换为您指定的dtype。&lt;br>
这里的关键在于：.to() 方法只会转换浮点类型的张量（例如 bfloat16, float16, float64）。&lt;br>
它会跳过那些已经是**整型（Integer）**的张量，比如我们量化后存储为torch.qint8的权重。
因此，INT8权重本身保持不变，量化的成果得以保留。&lt;/p>
&lt;h4 id="那我们为什么要执行这个转换呢">那我们为什么要执行这个转换呢？
&lt;/h4>&lt;p>这个操作的目的，正是为了解决您在上一个问题中遇到的 &lt;code>RuntimeError: expected scalar type Float but found BFloat16&lt;/code> 错误。&lt;br>
让我们回顾一下当时的情况：&lt;br>
(1)&lt;strong>原始模型&lt;/strong>：您加载模型时，因为设置了&lt;code>'dataformat': 'bf16'&lt;/code>，所以模型中所有的浮点参数都是&lt;code>bfloat16&lt;/code>类型。这包括&lt;code>Embedding&lt;/code>层、&lt;code>LayerNorm&lt;/code>层，以及&lt;code>Linear&lt;/code>层的权重和偏置。
(2)&lt;strong>量化之后&lt;/strong>：&lt;code>quantize_dynamic&lt;/code>函数只替换了&lt;code>Linear&lt;/code>层。但是，模型中的&lt;strong>其他层&lt;/strong>（如&lt;code>Embedding&lt;/code>和&lt;code>LayerNorm&lt;/code>）的参数&lt;strong>依然是&lt;code>bfloat16&lt;/code>类型&lt;/strong>。
(3)&lt;strong>冲突发生&lt;/strong>：当数据流经模型时，&lt;code>Embedding&lt;/code>层输出一个&lt;code>bfloat16&lt;/code>的张量，这个张量被送入到我们新替换的&lt;code>DynamicQuantizedLinear&lt;/code>层。然而，这个量化层的CPU后端实现&lt;strong>期望接收一个标准的&lt;code>float32&lt;/code>张量&lt;/strong>作为输入，而不是&lt;code>bfloat16&lt;/code>。于是，类型不匹配，程序崩溃。&lt;/p>
&lt;p>&lt;strong>所以，调用&lt;code>model.to(torch.float32)&lt;/code>的真正目的，是将模型中&lt;/strong>除了INT8权重之外的所有其他浮点参数和缓冲区**（比如仍然是&lt;code>bfloat16&lt;/code>格式的&lt;code>Embedding&lt;/code>层或&lt;code>LayerNorm&lt;/code>层）全部统一转换为标准的&lt;code>float32&lt;/code>格式。&lt;/p>
&lt;p>&lt;strong>总结一下，这个操作可以理解为一次“内务整理”：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>INT8权重&lt;/strong>： untouchable，保持量化状态。&lt;/li>
&lt;li>&lt;strong>其他所有浮点部分（&lt;code>bfloat16&lt;/code>）&lt;/strong>：全部转换为&lt;code>float32&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>这样一来，整个模型的数据流就统一了：非量化层输出&lt;code>float32&lt;/code>张量，量化层接收&lt;code>float32&lt;/code>张量，内部进行计算，从而完美解决了类型冲突的问题，同时保留了INT8量化带来的内存节省优势。&lt;/p>
&lt;h4 id="python中的enumereate">python中的enumereate
&lt;/h4>&lt;p>(1) 没有 &lt;code>enumerate&lt;/code> 的情况
您的原始循环是这样的：
&lt;code>for begin_loc in tqdm(range(0, 1536, 512)):&lt;/code>&lt;/p>
&lt;p>这个循环会执行三次，在每一次循环中：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第1次循环&lt;/strong>：&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>0&lt;/strong>&lt;/li>
&lt;li>&lt;strong>第2次循环&lt;/strong>：&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>512&lt;/strong>&lt;/li>
&lt;li>&lt;strong>第3次循环&lt;/strong>：&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>1024&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>可以看到，这个循环只提供了一个变量 &lt;code>begin_loc&lt;/code>。&lt;/p>
&lt;p>(2) 加入 &lt;code>enumerate&lt;/code> 之后的情况&lt;/p>
&lt;p>您修改后的正确循环是这样的：
&lt;code>for idx, begin_loc in enumerate(tqdm(range(0, 1536, 512))):&lt;/code>&lt;/p>
&lt;p>这个循环同样会执行三次，在每一次循环中：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第1次循环&lt;/strong>：&lt;code>idx&lt;/code> 的值是 &lt;strong>0&lt;/strong>，&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>0&lt;/strong>&lt;/li>
&lt;li>&lt;strong>第2次循环&lt;/strong>：&lt;code>idx&lt;/code> 的值是 &lt;strong>1&lt;/strong>，&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>512&lt;/strong>&lt;/li>
&lt;li>&lt;strong>第3次循环&lt;/strong>：&lt;code>idx&lt;/code> 的值是 &lt;strong>2&lt;/strong>，&lt;code>begin_loc&lt;/code> 的值是 &lt;strong>1024&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>结论
通过对比可以清晰地看到：&lt;br>
&lt;strong>核心变量 &lt;code>begin_loc&lt;/code> 的值完全没变&lt;/strong>：在每次对应的循环中，&lt;code>begin_loc&lt;/code> 得到的值和原来一模一样，都是 &lt;code>0, 512, 1024, ...&lt;/code>。所以您循环内部使用 &lt;code>begin_loc&lt;/code> 的所有逻辑都不会受到任何影响。&lt;br>
&lt;strong>循环次数完全没变&lt;/strong>：&lt;code>range&lt;/code> 函数生成多少个数字，循环就会执行多少次。&lt;code>enumerate&lt;/code> 不会增加或减少循环的次数。&lt;br>
&lt;strong>唯一的改变&lt;/strong>：&lt;code>enumerate&lt;/code> 只是为您&lt;strong>额外提供&lt;/strong>了一个从0开始的计数器变量 &lt;code>idx&lt;/code>。这个变量正是您在 &lt;code>print&lt;/code> 语句中所需要的，它解决了 &lt;code>NameError&lt;/code>，并且让您能方便地知道当前是第几步循环。&lt;br>
所以，加入 &lt;code>enumerate&lt;/code> 是一个纯粹的“增强”操作，它在不干扰原有功能的基础上，安全地添加了您需要的新功能（即步数索引）。这是在Python中进行带索引循环的标准且推荐的做法。&lt;/p>
&lt;h4 id="如果计算rwkv中的困惑度的呢">如果计算RWKV中的困惑度的呢
&lt;/h4>&lt;p>核心思想是**“对比每一个字（Token）的输出与下一个真实字的关系”**&lt;br>
RWKV的困惑度计算的核心代码（先计算loss）：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">forward_parallel_slices&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slice_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1024&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="只进行推理计算不计算梯度--二者有什么区别吗">只进行推理计算，不计算梯度 二者有什么区别吗
&lt;/h4>&lt;p>训练是为了教会模型，而推理是为了使用已经教好的模型&lt;br>
当然有，这两种模式在计算流程、资源消耗和最终目的上有着本质的区别。&lt;/p>
&lt;p>简单来说，&lt;strong>训练&lt;/strong>是为了&lt;strong>教会&lt;/strong>模型，而&lt;strong>推理&lt;/strong>是为了&lt;strong>使用&lt;/strong>已经教好的模型。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">特性 (Characteristic)&lt;/th>
&lt;th style="text-align: left">训练模式 (Training Mode)&lt;/th>
&lt;th style="text-align: left">推理模式 (Inference Mode)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>核心目标&lt;/strong>&lt;/td>
&lt;td style="text-align: left">&lt;strong>学习和优化&lt;/strong>模型参数（权重），让模型变得更“聪明”。&lt;/td>
&lt;td style="text-align: left">&lt;strong>使用&lt;/strong>已训练好的模型对新数据进行&lt;strong>预测&lt;/strong>和&lt;strong>评估&lt;/strong>。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>计算流程&lt;/strong>&lt;/td>
&lt;td style="text-align: left">&lt;strong>前向传播 + 反向传播&lt;/strong> (Forward Pass + Backward Pass)&lt;/td>
&lt;td style="text-align: left">&lt;strong>仅有前向传播&lt;/strong> (Forward Pass ONLY)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>梯度计算&lt;/strong>&lt;/td>
&lt;td style="text-align: left">&lt;strong>必须计算梯度&lt;/strong>。梯度是反向传播的核心，它告诉每个权重应该如何调整才能减小误差（loss）。&lt;/td>
&lt;td style="text-align: left">&lt;strong>完全不需要计算梯度&lt;/strong>。因为模型参数是固定的，我们不打算更新它，所以计算梯度就成了不必要的开销。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>内存占用&lt;/strong>&lt;/td>
&lt;td style="text-align: left">&lt;strong>高&lt;/strong>。为了能够进行反向传播，PyTorch需要构建一个“计算图”，并&lt;strong>保存所有中间步骤的计算结果&lt;/strong>（激活值）。这会占用大量内存。&lt;/td>
&lt;td style="text-align: left">&lt;strong>低&lt;/strong>。因为不需要反向传播，PyTorch可以&lt;strong>用完一个中间结果后立即丢弃&lt;/strong>，无需为计算图保存任何额外信息，大大节省了内存。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>计算速度&lt;/strong>&lt;/td>
&lt;td style="text-align: left">&lt;strong>慢&lt;/strong>。既要跑一遍前向传播，又要跑一遍更复杂的反向传播。&lt;/td>
&lt;td style="text-align: left">&lt;strong>快&lt;/strong>。只执行一次前向传播，省去了所有与梯度相关的计算。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>在PyTorch中的体现&lt;/strong>&lt;/td>
&lt;td style="text-align: left">默认模式。执行&lt;code>loss.backward()&lt;/code>和&lt;code>optimizer.step()&lt;/code>来更新权重。&lt;/td>
&lt;td style="text-align: left">使用&lt;code>with torch.no_grad():&lt;/code>代码块包裹。同时常配合&lt;code>model.eval()&lt;/code>来关闭Dropout等只在训练时使用的层。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因此，在计算困惑度时，使用 &lt;code>with torch.no_grad():&lt;/code> 是一个重要的优化步骤，它能让评估过程变得更快、更节省资源。&lt;/p></description></item></channel></rss>