<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="疑惑 Pytorch平台上对RWKV的困惑度计算模块进行量化的经历 Q:更稳妥的方式是在模型加载和量化后，将整个模型转为float32，再量化后转换成float32的目的是什么呢，这样不是把原来量化好的给毁了吗\nA：\n"><title>Quantization</title><link rel=canonical href=https://etherealzoom.github.io/blog/p/quantization/><link rel=stylesheet href=/blog/scss/style.min.232425a77efba02f6f871cbb14abca94f2b60d5f9313c526b1f80bdec16dc24f.css><meta property='og:title' content="Quantization"><meta property='og:description' content="疑惑 Pytorch平台上对RWKV的困惑度计算模块进行量化的经历 Q:更稳妥的方式是在模型加载和量化后，将整个模型转为float32，再量化后转换成float32的目的是什么呢，这样不是把原来量化好的给毁了吗\nA：\n"><meta property='og:url' content='https://etherealzoom.github.io/blog/p/quantization/'><meta property='og:site_name' content='etherealzoom'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-07-09T23:06:03+08:00'><meta property='article:modified_time' content='2025-07-09T23:06:03+08:00'><meta name=twitter:title content="Quantization"><meta name=twitter:description content="疑惑 Pytorch平台上对RWKV的困惑度计算模块进行量化的经历 Q:更稳妥的方式是在模型加载和量化后，将整个模型转为float32，再量化后转换成float32的目的是什么呢，这样不是把原来量化好的给毁了吗\nA：\n"><link rel="shortcut icon" href=/blog/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_69af1f53ef5879c1.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/blog>etherealzoom</a></h1><h2 class=site-description>Welcome!</h2></div></header><ol class=menu-social><li><a href='https://space.bilibili.com/441382883?spm_id_from=333.337.0.0' target=_blank title=bilibili rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-bilibili"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 10a4 4 0 014-4h10a4 4 0 014 4v6a4 4 0 01-4 4H7a4 4 0 01-4-4v-6z"/><path d="M8 3l2 3"/><path d="M16 3l-2 3"/><path d="M9 13v-2"/><path d="M15 11v2"/></svg></a></li><li><a href=https://github.com/etherealzoom target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg>
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-moon-stars"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/><path d="M17 4a2 2 0 002 2 2 2 0 00-2 2 2 2 0 00-2-2 2 2 0 002-2"/><path d="M19 11h2m-1-1v2"/></svg>
<span class=label-light>浅色模式</span> <span class=label-dark>深色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#疑惑>疑惑</a><ol><li><a href=#pytorch平台上对rwkv的困惑度计算模块进行量化的经历>Pytorch平台上对RWKV的困惑度计算模块进行量化的经历</a><ol><li><a href=#pytorch上量化做了什么>pytorch上量化做了什么</a></li><li><a href=#totorchfloat32-到底做了什么>.to(torch.float32) 到底做了什么？</a></li><li><a href=#那我们为什么要执行这个转换呢>那我们为什么要执行这个转换呢？</a></li><li><a href=#python中的enumereate>python中的enumereate</a></li><li><a href=#如果计算rwkv中的困惑度的呢>如果计算RWKV中的困惑度的呢</a></li><li><a href=#只进行推理计算不计算梯度--二者有什么区别吗>只进行推理计算，不计算梯度 二者有什么区别吗</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/quantization/>Quantization</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 09, 2025</time></div></footer></div></header><section class=article-content><h2 id=疑惑>疑惑</h2><h3 id=pytorch平台上对rwkv的困惑度计算模块进行量化的经历>Pytorch平台上对RWKV的困惑度计算模块进行量化的经历</h3><p>Q:更稳妥的方式是在模型加载和量化后，将整个模型转为float32，再量化后转换成float32的目的是什么呢，这样不是把原来量化好的给毁了吗<br>A：</p><h4 id=pytorch上量化做了什么>pytorch上量化做了什么</h4><p>当调用torch.quantization.quantize_dynamic时，PyTorch并不仅仅是把权重张量的数据类型（dtype）改了一下。它做了一件更根本的事：它将模型中的标准nn.Linear层替换成了一个全新的、专门的模块，叫做DynamicQuantizedLinear。<br>在这个新的DynamicQuantizedLinear模块内部，权重确实被压缩并以**torch.qint8（8位整型）**的格式存储了起来。<br>同时，这个模块还保存了将这些INT8整数反量化回浮点数所需的缩放因子（scale）和零点（zero-point）（它们是浮点数）。</p><h4 id=totorchfloat32-到底做了什么>.to(torch.float32) 到底做了什么？</h4><p>当您对一个模型调用.to(dtype)方法时，PyTorch会遍历模型的所有参数（parameters）和缓冲区（buffers），并尝试将它们的类型转换为您指定的dtype。<br>这里的关键在于：.to() 方法只会转换浮点类型的张量（例如 bfloat16, float16, float64）。<br>它会跳过那些已经是**整型（Integer）**的张量，比如我们量化后存储为torch.qint8的权重。
因此，INT8权重本身保持不变，量化的成果得以保留。</p><h4 id=那我们为什么要执行这个转换呢>那我们为什么要执行这个转换呢？</h4><p>这个操作的目的，正是为了解决您在上一个问题中遇到的 <code>RuntimeError: expected scalar type Float but found BFloat16</code> 错误。<br>让我们回顾一下当时的情况：<br>(1)<strong>原始模型</strong>：您加载模型时，因为设置了<code>'dataformat': 'bf16'</code>，所以模型中所有的浮点参数都是<code>bfloat16</code>类型。这包括<code>Embedding</code>层、<code>LayerNorm</code>层，以及<code>Linear</code>层的权重和偏置。
(2)<strong>量化之后</strong>：<code>quantize_dynamic</code>函数只替换了<code>Linear</code>层。但是，模型中的<strong>其他层</strong>（如<code>Embedding</code>和<code>LayerNorm</code>）的参数<strong>依然是<code>bfloat16</code>类型</strong>。
(3)<strong>冲突发生</strong>：当数据流经模型时，<code>Embedding</code>层输出一个<code>bfloat16</code>的张量，这个张量被送入到我们新替换的<code>DynamicQuantizedLinear</code>层。然而，这个量化层的CPU后端实现<strong>期望接收一个标准的<code>float32</code>张量</strong>作为输入，而不是<code>bfloat16</code>。于是，类型不匹配，程序崩溃。</p><p><strong>所以，调用<code>model.to(torch.float32)</code>的真正目的，是将模型中</strong>除了INT8权重之外的所有其他浮点参数和缓冲区**（比如仍然是<code>bfloat16</code>格式的<code>Embedding</code>层或<code>LayerNorm</code>层）全部统一转换为标准的<code>float32</code>格式。</p><p><strong>总结一下，这个操作可以理解为一次“内务整理”：</strong></p><ul><li><strong>INT8权重</strong>： untouchable，保持量化状态。</li><li><strong>其他所有浮点部分（<code>bfloat16</code>）</strong>：全部转换为<code>float32</code>。</li></ul><p>这样一来，整个模型的数据流就统一了：非量化层输出<code>float32</code>张量，量化层接收<code>float32</code>张量，内部进行计算，从而完美解决了类型冲突的问题，同时保留了INT8量化带来的内存节省优势。</p><h4 id=python中的enumereate>python中的enumereate</h4><p>(1) 没有 <code>enumerate</code> 的情况
您的原始循环是这样的：
<code>for begin_loc in tqdm(range(0, 1536, 512)):</code></p><p>这个循环会执行三次，在每一次循环中：</p><ul><li><strong>第1次循环</strong>：<code>begin_loc</code> 的值是 <strong>0</strong></li><li><strong>第2次循环</strong>：<code>begin_loc</code> 的值是 <strong>512</strong></li><li><strong>第3次循环</strong>：<code>begin_loc</code> 的值是 <strong>1024</strong></li></ul><p>可以看到，这个循环只提供了一个变量 <code>begin_loc</code>。</p><p>(2) 加入 <code>enumerate</code> 之后的情况</p><p>您修改后的正确循环是这样的：
<code>for idx, begin_loc in enumerate(tqdm(range(0, 1536, 512))):</code></p><p>这个循环同样会执行三次，在每一次循环中：</p><ul><li><strong>第1次循环</strong>：<code>idx</code> 的值是 <strong>0</strong>，<code>begin_loc</code> 的值是 <strong>0</strong></li><li><strong>第2次循环</strong>：<code>idx</code> 的值是 <strong>1</strong>，<code>begin_loc</code> 的值是 <strong>512</strong></li><li><strong>第3次循环</strong>：<code>idx</code> 的值是 <strong>2</strong>，<code>begin_loc</code> 的值是 <strong>1024</strong></li></ul><p>结论
通过对比可以清晰地看到：<br><strong>核心变量 <code>begin_loc</code> 的值完全没变</strong>：在每次对应的循环中，<code>begin_loc</code> 得到的值和原来一模一样，都是 <code>0, 512, 1024, ...</code>。所以您循环内部使用 <code>begin_loc</code> 的所有逻辑都不会受到任何影响。<br><strong>循环次数完全没变</strong>：<code>range</code> 函数生成多少个数字，循环就会执行多少次。<code>enumerate</code> 不会增加或减少循环的次数。<br><strong>唯一的改变</strong>：<code>enumerate</code> 只是为您<strong>额外提供</strong>了一个从0开始的计数器变量 <code>idx</code>。这个变量正是您在 <code>print</code> 语句中所需要的，它解决了 <code>NameError</code>，并且让您能方便地知道当前是第几步循环。<br>所以，加入 <code>enumerate</code> 是一个纯粹的“增强”操作，它在不干扰原有功能的基础上，安全地添加了您需要的新功能（即步数索引）。这是在Python中进行带索引循环的标准且推荐的做法。</p><h4 id=如果计算rwkv中的困惑度的呢>如果计算RWKV中的困惑度的呢</h4><p>核心思想是**“对比每一个字（Token）的输出与下一个真实字的关系”**<br>RWKV的困惑度计算的核心代码（先计算loss）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>,</span> <span class=n>state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>forward_parallel_slices</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>slice_len</span><span class=o>=</span><span class=mi>1024</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span> <span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=只进行推理计算不计算梯度--二者有什么区别吗>只进行推理计算，不计算梯度 二者有什么区别吗</h4><p>训练是为了教会模型，而推理是为了使用已经教好的模型<br>当然有，这两种模式在计算流程、资源消耗和最终目的上有着本质的区别。</p><p>简单来说，<strong>训练</strong>是为了<strong>教会</strong>模型，而<strong>推理</strong>是为了<strong>使用</strong>已经教好的模型。</p><div class=table-wrapper><table><thead><tr><th style=text-align:left>特性 (Characteristic)</th><th style=text-align:left>训练模式 (Training Mode)</th><th style=text-align:left>推理模式 (Inference Mode)</th></tr></thead><tbody><tr><td style=text-align:left><strong>核心目标</strong></td><td style=text-align:left><strong>学习和优化</strong>模型参数（权重），让模型变得更“聪明”。</td><td style=text-align:left><strong>使用</strong>已训练好的模型对新数据进行<strong>预测</strong>和<strong>评估</strong>。</td></tr><tr><td style=text-align:left><strong>计算流程</strong></td><td style=text-align:left><strong>前向传播 + 反向传播</strong> (Forward Pass + Backward Pass)</td><td style=text-align:left><strong>仅有前向传播</strong> (Forward Pass ONLY)</td></tr><tr><td style=text-align:left><strong>梯度计算</strong></td><td style=text-align:left><strong>必须计算梯度</strong>。梯度是反向传播的核心，它告诉每个权重应该如何调整才能减小误差（loss）。</td><td style=text-align:left><strong>完全不需要计算梯度</strong>。因为模型参数是固定的，我们不打算更新它，所以计算梯度就成了不必要的开销。</td></tr><tr><td style=text-align:left><strong>内存占用</strong></td><td style=text-align:left><strong>高</strong>。为了能够进行反向传播，PyTorch需要构建一个“计算图”，并<strong>保存所有中间步骤的计算结果</strong>（激活值）。这会占用大量内存。</td><td style=text-align:left><strong>低</strong>。因为不需要反向传播，PyTorch可以<strong>用完一个中间结果后立即丢弃</strong>，无需为计算图保存任何额外信息，大大节省了内存。</td></tr><tr><td style=text-align:left><strong>计算速度</strong></td><td style=text-align:left><strong>慢</strong>。既要跑一遍前向传播，又要跑一遍更复杂的反向传播。</td><td style=text-align:left><strong>快</strong>。只执行一次前向传播，省去了所有与梯度相关的计算。</td></tr><tr><td style=text-align:left><strong>在PyTorch中的体现</strong></td><td style=text-align:left>默认模式。执行<code>loss.backward()</code>和<code>optimizer.step()</code>来更新权重。</td><td style=text-align:left>使用<code>with torch.no_grad():</code>代码块包裹。同时常配合<code>model.eval()</code>来关闭Dropout等只在训练时使用的层。</td></tr></tbody></table></div><p>因此，在计算困惑度时，使用 <code>with torch.no_grad():</code> 是一个重要的优化步骤，它能让评估过程变得更快、更节省资源。</p></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Copyright © 2025 etherealzoom</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><script src=https://utteranc.es/client.js repo issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2025 etherealzoom</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>